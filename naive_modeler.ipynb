{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Naive Modeler\n",
    "**Expectation:**  \n",
    "It is sometimes suggested that machine learning models are so \"intelligent\" that you simply have to plug training data into a model to fit it and you should get pretty much as good a model as can be achieved. Perhaps some playing around with the very basic model parameters might be needed but that's all.\n",
    "\n",
    "**Reality**  \n",
    "Often, such naive model building does not yield very good models. There are many reasons why this might happen. For example:\n",
    "* the presence of interaction effects\n",
    "* the need for calculation of appropriate features from the raw data\n",
    "* cleaning of the data would be needed and requires in-depth knowledge of the data\n",
    "* ...\n",
    "\n",
    "**This Project**  \n",
    "The purpose of this project is simply to investigate how well common predictive regression model types can handle significant interaction between predictors. The code uses randomly generated data and builds regression models of the most common types, sometimes trying multiple simple variations. It assumes that the modeler is not attempting to understand the data very closely, is not going to calculate new features and will only attempt to try the most basic variations of the predictive regression model types.\n",
    "\n",
    "**The Data**  \n",
    "The data will contain box dimensions (length, width and height) generated randomly, each between 0 and 1.0. These are the predictors. The outcome is the box volume. There is no error term present so perfect predictions are possible. The code sets aside 10% of the data as the test data, leaving 90% as training data.\n",
    "\n",
    "**The Models**  \n",
    "Various regression model types are built. A model predicting the volume according to: \n",
    "predicted_volume = length x width x height \n",
    "would achieve 100% accuracy. However, most model types cannot predict using a relationship like this and so will be less than 100% accurate. Sinde the assumption is that the user is naive, the code does not calculate dimensions_product = length x width x height as a new feature which is what a modeler who understands the data and the abilities of various model types would do.\n",
    "\n",
    "**Library Requirements**  \n",
    "numpy, scikit-learn(sklearn) and (for the last models) tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#set the random seed so models and results can be reproduced\n",
    "np.random.seed(seed=123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants - you can try out different values for these\n",
    "NUM_BOXES = 10000        # recommended at least 1000 - if too low models might suffer from overfitting\n",
    "TRAINING_PERCENT = 90    # 90 is recommended, keep between 1 and 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array with columns being the 3 dimensions of boxes (length, width, height)\n",
    "# which are random values from (0,1]\n",
    "# Note: random_sample generates values in range [0,1) so multiply by -1 and add 1 to get in range (0,1\n",
    "box_dims = np.random.random_sample(size=(NUM_BOXES, 3))\n",
    "box_dims = (box_dims * (-1)) + 1\n",
    "# create an array with the corresponding volumes of the boxes\n",
    "box_volumes = box_dims[:,0] * box_dims[:,1] * box_dims[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With box volumes as the outcome, there is a strong interaction effect because the contribution to outcome value from each dimension depends on the other. For example, an additional 0.1 added to the length of a box increases the volume of a box with width 0.8 and height 0.8 than it would add to a box with width 0.2 and height 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditionally the predictors array is called X and the outcome array is y\n",
    "X = box_dims\n",
    "y = box_volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, if y is instead the length of tape around the box (2\\*width + 2\\*height) then there is no interaction effect. In this case many of the models below should make perfect (or almost perfect) predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following to try tape length as the outcome.\n",
    "#box_tape = 2*box_dims[:,1] + 2*box_dims[:,2]\n",
    "#y = box_tape\n",
    "# Any reference below to \"volume\" should be understood to mean \"tape length\" if the last lines are uncommented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training set and a test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=TRAINING_PERCENT/100, random_state=987654321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    shape X: (10000, 3)\n",
      "    shape y: (10000,)\n",
      "    shape X_train: (9000, 3)\n",
      "    shape X_test: (1000, 3)\n",
      "    shape y_train: (9000,)\n",
      "    shape y_test: (1000,)\n",
      "    mean X: 0.4994\n",
      "    mean y: 1.9956\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n\\\n",
    "    shape X: {X.shape}\\n\\\n",
    "    shape y: {y.shape}\\n\\\n",
    "    shape X_train: {X_train.shape}\\n\\\n",
    "    shape X_test: {X_test.shape}\\n\\\n",
    "    shape y_train: {y_train.shape}\\n\\\n",
    "    shape y_test: {y_test.shape}\\n\\\n",
    "    mean X: {X.mean():.4f}\\n\\\n",
    "    mean y: {y.mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of curiosity, calculate correlations between the predictors, predictor products and the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the columns and calculate products of dimension pairs\n",
    "X_length = X[:,0]\n",
    "X_width = X[:,1]\n",
    "X_height = X[:,2]\n",
    "X_length_by_width = X_length * X_width\n",
    "X_length_by_height = X_length * X_height\n",
    "X_width_by_height = X_width * X_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2(var_1, var_2):\n",
    "    corrcoef_result = np.corrcoef(var_1, var_2)\n",
    "    # the result is the covariance matrix, element [1,0] is the correlation coefficient \n",
    "    return corrcoef_result[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between predictors and outcome and products of predictors and outcomes\n",
      "length vs volume: -0.0001\n",
      "width vs volume: 0.7058\n",
      "height vs volume: 0.7048\n",
      "(length x width) vs volume: 0.4588\n",
      "(length x height) vs volume: 0.4574\n",
      "(width x height) vs volume: 0.9261\n"
     ]
    }
   ],
   "source": [
    "print('Correlation between predictors and outcome and products of predictors and outcomes')\n",
    "print(f'length vs volume: {corr2(X_length,y):.4f}')\n",
    "print(f'width vs volume: {corr2(X_width,y):.4f}')\n",
    "print(f'height vs volume: {corr2(X_height,y):.4f}')\n",
    "print(f'(length x width) vs volume: {corr2(X_length_by_width,y):.4f}')\n",
    "print(f'(length x height) vs volume: {corr2(X_length_by_height,y):.4f}')\n",
    "print(f'(width x height) vs volume: {corr2(X_width_by_height,y):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a Normalized Root Mean Squared Error, as a percent, for measuring the accuracy of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# The smaller nrmse the more accurate - a nrmse of 0 means perfect accuracy.\n",
    "def pct_nrmse(y_actual, y_pred):\n",
    "    # normalize the rmse by dividing the rmse by the average predicted value\n",
    "    avg_pred = np.mean(y_pred)\n",
    "    rmse = sqrt(mean_squared_error(y_actual, y_pred))\n",
    "    return (rmse / avg_pred) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using the training data\n",
    "reg_model = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent error: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculatye the predicted values and measure accuracy\n",
    "y_pred_reg_model = reg_model.predict(X_test)\n",
    "pct_err = pct_nrmse(y_test, y_pred_reg_model)\n",
    "print(f'Percent error: {pct_err:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeffs: \n",
      "      0.0000(length), \n",
      "      2.0000(width), \n",
      "      2.0000(height). \n",
      "Intercept: -0.0000\n"
     ]
    }
   ],
   "source": [
    "# For interest, view the coefficients generated for the model\n",
    "print(f'Coeffs: \\n\\\n",
    "      {reg_model.coef_[0]:.4f}(length), \\n\\\n",
    "      {reg_model.coef_[1]:.4f}(width), \\n\\\n",
    "      {reg_model.coef_[2]:.4f}(height). \\n\\\n",
    "Intercept: {reg_model.intercept_:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = Ridge(alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "      normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent error: 0.05%\n"
     ]
    }
   ],
   "source": [
    "#Measure accuracy\n",
    "y_pred_ridge_model = ridge_model.predict(X_test)\n",
    "pct_err = pct_nrmse(y_test, y_pred_ridge_model)\n",
    "print(f'Percent error: {pct_err:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon-Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll use all types of kernels to compare\n",
    "#Returns the accuracy\n",
    "def svr_with_kernel(X_train, X_test, y_train, y_test, kernel):\n",
    "    svr_model = SVR(gamma='scale', kernel=kernel)\n",
    "    svr_model.fit(X_train, y_train)\n",
    "    y_pred_svr_model = svr_model.predict(X_test)\n",
    "    return pct_nrmse(y_test, y_pred_svr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR percent error, kernel: \n",
      "\n",
      "2.09%, linear\n",
      "13.03%, poly\n",
      "1.83%, rbf\n",
      "316.63%, sigmoid\n"
     ]
    }
   ],
   "source": [
    "kernels = ('linear', 'poly', 'rbf', 'sigmoid')\n",
    "\n",
    "print(f'SVR percent error, kernel: \\n')\n",
    "for kernel in kernels:\n",
    "    svr_acc = svr_with_kernel(X_train, X_test, y_train, y_test, kernel)\n",
    "    print(f'{svr_acc:.2f}%, {kernel}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use depths up to log base 2 of NUM_BOXES\n",
    "first_depth = 1\n",
    "depths = list(range(first_depth, int(math.log(NUM_BOXES, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll use all a variety of depths to compare\n",
    "#Returns a tuple containing the accuracy and the number of leaves\n",
    "def dec_tree_with_depth(X_train, X_test, y_train, y_test, depth):\n",
    "    dec_tree_model = DecisionTreeRegressor(max_depth=depth)\n",
    "    dec_tree_model.fit(X_train, y_train)\n",
    "    y_pred_dec_tree_model = dec_tree_model.predict(X_test)\n",
    "    dec_tree_rmse = pct_nrmse(y_test, y_pred_dec_tree_model)\n",
    "    num_leaves = dec_tree_model.get_n_leaves()\n",
    "    return (dec_tree_rmse, num_leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree: percent error, num leaves, depth:\n",
      "32.36%, 2 - depth=1\n",
      "20.83%, 4 - depth=2\n",
      "15.82%, 8 - depth=3\n",
      "10.66%, 16 - depth=4\n",
      "8.06%, 32 - depth=5\n",
      "5.50%, 64 - depth=6\n",
      "4.17%, 128 - depth=7\n",
      "2.84%, 256 - depth=8\n",
      "2.21%, 512 - depth=9\n",
      "1.66%, 1024 - depth=10\n",
      "1.34%, 1999 - depth=11\n",
      "1.23%, 3569 - depth=12\n",
      "1.14%, 8644 - depth=no limit\n"
     ]
    }
   ],
   "source": [
    "print(f'Decision tree: percent error, num leaves, depth:')\n",
    "for depth in depths:\n",
    "    dec_tree_depth = dec_tree_with_depth(X_train, X_test, y_train, y_test, depth)\n",
    "    print(f'{dec_tree_depth[0]:.2f}%, {dec_tree_depth[1]} - depth={depth}')\n",
    "    \n",
    "dec_tree_depth_no_limit = dec_tree_with_depth(X_train, X_test, y_train, y_test, None)\n",
    "print(f'{dec_tree_depth_no_limit[0]:.2f}%, {dec_tree_depth_no_limit[1]} - depth=no limit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_regressor_model = RandomForestRegressor(random_state=911)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3.7.3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                      n_jobs=None, oob_score=False, random_state=911, verbose=0,\n",
       "                      warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_regressor_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent error: 0.64%\n"
     ]
    }
   ],
   "source": [
    "y_pred_random_forest_regressor_model = random_forest_regressor_model.predict(X_test)\n",
    "pct_err = pct_nrmse(y_test, y_pred_random_forest_regressor_model)\n",
    "print(f'Percent error: {pct_err:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network 1 (using mostly sklearn defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll use all activation functions\n",
    "#Prints the name of the activation function, the accuracy and the number of layers\n",
    "def mlp_with_act_func(X_train, X_test, y_train, y_test, act_func):\n",
    "    mlp_model = MLPRegressor(activation=act_func, hidden_layer_sizes=(9,9,9))\n",
    "    mlp_model.fit(X_train, y_train)\n",
    "    y_pred_mlp_model = mlp_model.predict(X_test)\n",
    "    mlp_model_pct_nrmse = pct_nrmse(y_test, y_pred_mlp_model)\n",
    "    mlp_model_num_layers = mlp_model.n_layers_\n",
    "    print(f'{act_func}, {mlp_model_pct_nrmse:.2f}%, {mlp_model_num_layers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation, percent error, num layers\n",
      "identity, 0.00%, 5\n",
      "logistic, 1.58%, 5\n",
      "tanh, 2.61%, 5\n",
      "relu, 1.15%, 5\n"
     ]
    }
   ],
   "source": [
    "act_functions = ('identity', 'logistic', 'tanh', 'relu')\n",
    "\n",
    "print('activation, percent error, num layers')\n",
    "for act_func in act_functions:\n",
    "    mlp_with_act_func(X_train, X_test, y_train, y_test, act_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network 2 - using Tensorflow, only input layer and output layer, variety of activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = ('relu', 'sigmoid', 'tanh', 'linear', 'exponential', 'softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs simple NN, no hidden layers, only input layer and output layer\n",
    "#returns accuracy\n",
    "def simple_NN(X_train, X_test, y_train, y_test, activation):\n",
    "    nn_model = keras.Sequential()\n",
    "    nn_model.add(Dense(1, input_dim=(3), activation=activation))\n",
    "    #optimizer = tf.keras.optimizers.SGD(0.001)\n",
    "    nn_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "    nn_model.fit(X_train, y_train, batch_size=10, epochs=100, verbose=0)\n",
    "    y_pred_nn_model = nn_model.predict(X_test)\n",
    "    nn_model_pct_nrmse = pct_nrmse(y_test, y_pred_nn_model)\n",
    "    print(f'{activation}, {nn_model_pct_nrmse:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation, accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3.7.3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu, inf%\n",
      "sigmoid, 131.70%\n",
      "tanh, 131.26%\n",
      "linear, 0.00%\n",
      "exponential, 9.00%\n",
      "softmax, 127.06%\n"
     ]
    }
   ],
   "source": [
    "print('activation, accuracy')\n",
    "\n",
    "for act in activations:\n",
    "    simple_NN(X_train, X_test, y_train, y_test, act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network 3 - 3 hidden layers, all layers using the same specified activation function, variety of activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs a NN with 3 hidden layers, all layers using the same specified activation function\n",
    "#prints the activation and the accuracy\n",
    "def layered_NN(X_train, X_test, y_train, y_test, activation):\n",
    "    nn_model = keras.Sequential()\n",
    "    nn_model.add(Dense(6, input_dim=(3), activation=activation))\n",
    "    nn_model.add(Dense(6, activation=activation))\n",
    "    nn_model.add(Dense(6, activation=activation))\n",
    "    nn_model.add(Dense(1, activation=activation))\n",
    "    optimizer = tf.keras.optimizers.SGD(0.001)\n",
    "    nn_model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mean_squared_error'])\n",
    "    nn_model.fit(X_train, y_train, batch_size=10, epochs=100, verbose=0)\n",
    "    y_pred_nn_model = nn_model.predict(X_test)\n",
    "    nn_model_pct_nrmse = pct_nrmse(y_test, y_pred_nn_model)\n",
    "    print(f'{activation}, {nn_model_pct_nrmse:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation, accuracy\n",
      "relu, 0.58%\n",
      "sigmoid, 127.33%\n",
      "tanh, 127.11%\n",
      "linear, 0.00%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-0364b5bed946>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mact\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mlayered_NN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-45-81c714a4b291>\u001b[0m in \u001b[0;36mlayered_NN\u001b[1;34m(X_train, X_test, y_train, y_test, activation)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0my_pred_nn_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mnn_model_pct_nrmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpct_nrmse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_nn_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{activation}, {nn_model_pct_nrmse:.2f}%'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-a68d9a0cd79b>\u001b[0m in \u001b[0;36mpct_nrmse\u001b[1;34m(y_actual, y_pred)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# normalize the rmse by dividing the rmse by the average predicted value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mavg_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mrmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_actual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrmse\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mavg_pred\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3.7.3\\lib\\site-packages\\sklearn\\metrics\\regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \"\"\"\n\u001b[0;32m    240\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[1;32m--> 241\u001b[1;33m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[0;32m    242\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "\u001b[1;32mc:\\python3.7.3\\lib\\site-packages\\sklearn\\metrics\\regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3.7.3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 542\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3.7.3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan)\u001b[0m\n\u001b[0;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "print('activation, accuracy')\n",
    "\n",
    "for act in activations:\n",
    "    layered_NN(X_train, X_test, y_train, y_test, act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation, accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3.7.3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu, inf%\n",
      "sigmoid, 127.42%\n",
      "tanh, 127.11%\n",
      "linear, 41.01%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-0d341c093cf3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mact\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mwith_power_activations_NN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-48-0a85f75b22a2>\u001b[0m in \u001b[0;36mwith_power_activations_NN\u001b[1;34m(X_train, X_test, y_train, y_test, activation)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0my_pred_nn_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mnn_model_pct_nrmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpct_nrmse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_nn_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{activation}, {nn_model_pct_nrmse:.2f}%'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-a68d9a0cd79b>\u001b[0m in \u001b[0;36mpct_nrmse\u001b[1;34m(y_actual, y_pred)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# normalize the rmse by dividing the rmse by the average predicted value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mavg_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mrmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_actual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrmse\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mavg_pred\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3.7.3\\lib\\site-packages\\sklearn\\metrics\\regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \"\"\"\n\u001b[0;32m    240\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[1;32m--> 241\u001b[1;33m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[0;32m    242\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "\u001b[1;32mc:\\python3.7.3\\lib\\site-packages\\sklearn\\metrics\\regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3.7.3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 542\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3.7.3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan)\u001b[0m\n\u001b[0;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "print('activation, accuracy')\n",
    "\n",
    "for act in activations:\n",
    "    with_power_activations_NN(X_train, X_test, y_train, y_test, act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
